{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "# Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProviderID</th>\n",
       "      <th>PolicyholderID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>DOB</th>\n",
       "      <th>DOD</th>\n",
       "      <th>DeductiblePaid</th>\n",
       "      <th>ClaimPayment</th>\n",
       "      <th>ClaimStartDate</th>\n",
       "      <th>ClaimEndDate</th>\n",
       "      <th>ClaimDuration</th>\n",
       "      <th>...</th>\n",
       "      <th>Cancer</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Arthritis</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>ReimburseIP</th>\n",
       "      <th>ReimburseOP</th>\n",
       "      <th>DeductibleIP</th>\n",
       "      <th>DeductibleOP</th>\n",
       "      <th>ProbableFraud</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51001</td>\n",
       "      <td>56354</td>\n",
       "      <td>Female</td>\n",
       "      <td>1934-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51001</td>\n",
       "      <td>106078</td>\n",
       "      <td>Female</td>\n",
       "      <td>1942-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51001</td>\n",
       "      <td>137197</td>\n",
       "      <td>Male</td>\n",
       "      <td>1964-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>2023-04-22</td>\n",
       "      <td>2023-04-22</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2560</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51001</td>\n",
       "      <td>38773</td>\n",
       "      <td>Male</td>\n",
       "      <td>1952-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1068</td>\n",
       "      <td>12000</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95000</td>\n",
       "      <td>2270</td>\n",
       "      <td>2136</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51001</td>\n",
       "      <td>32715</td>\n",
       "      <td>Male</td>\n",
       "      <td>1950-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>6700</td>\n",
       "      <td>1068</td>\n",
       "      <td>2700</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProviderID  PolicyholderID     Sex         DOB  DOD  DeductiblePaid  \\\n",
       "0       51001           56354  Female  1934-06-01  NaN               0   \n",
       "1       51001          106078  Female  1942-12-01  NaN               0   \n",
       "2       51001          137197    Male  1964-01-01  NaN               0   \n",
       "3       51001           38773    Male  1952-05-01  NaN            1068   \n",
       "4       51001           32715    Male  1950-03-01  NaN               0   \n",
       "\n",
       "   ClaimPayment ClaimStartDate ClaimEndDate  ClaimDuration  ...  Cancer  \\\n",
       "0           100     2023-06-08   2023-06-08              0  ...       0   \n",
       "1            90     2023-06-16   2023-06-16              0  ...       0   \n",
       "2          1500     2023-04-22   2023-04-22              0  ...       1   \n",
       "3         12000     2023-05-23   2023-05-25              2  ...       0   \n",
       "4           500     2023-03-29   2023-03-30              1  ...       1   \n",
       "\n",
       "   Depression  Arthritis  Stroke ReimburseIP ReimburseOP DeductibleIP  \\\n",
       "0           0          0       0           0         320            0   \n",
       "1           0          0       0           0         190            0   \n",
       "2           1          1       0           0        2560            0   \n",
       "3           0          0       0       95000        2270         2136   \n",
       "4           1          0       0        2020        6700         1068   \n",
       "\n",
       "   DeductibleOP ProbableFraud ID  \n",
       "0            80             0  1  \n",
       "1            20             0  2  \n",
       "2           480             0  3  \n",
       "3           900             0  4  \n",
       "4          2700             0  5  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'health_data.csv'\n",
    "health_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display data\n",
    "health_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 258. MiB for an array with shape (2430, 111418) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Encode categorical variables using label encoding\u001b[39;00m\n\u001b[0;32m     28\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProviderID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicyholderID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttendingID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeFirst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeSecond\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     29\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeThird\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeHospital\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 31\u001b[0m health_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhealth_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m health_data\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Checking the dataset after preprocessing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\encoding.py:192\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    188\u001b[0m     with_dummies \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39mdtypes_to_encode)]\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (col, pre, sep) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_to_encode\u001b[38;5;241m.\u001b[39mitems(), prefix, prefix_sep):\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# col is (column_name, column), use just column data here\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     with_dummies\u001b[38;5;241m.\u001b[39mappend(dummy)\n\u001b[0;32m    202\u001b[0m result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\encoding.py:311\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m concat(sparse_series, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# take on axis=1 + transpose to ensure ndarray layout is column-major\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n\u001b[0;32m    315\u001b[0m         dummy_mat[codes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 258. MiB for an array with shape (2430, 111418) and data type uint8"
     ]
    }
   ],
   "source": [
    "# Convert dates to datetime\n",
    "health_data['DOB'] = pd.to_datetime(health_data['DOB'], errors='coerce')\n",
    "health_data['ClaimStartDate'] = pd.to_datetime(health_data['ClaimStartDate'], errors='coerce')\n",
    "health_data['ClaimEndDate'] = pd.to_datetime(health_data['ClaimEndDate'], errors='coerce')\n",
    "\n",
    "# Calculate Age at Claim\n",
    "health_data['AgeAtClaim'] = (health_data['ClaimStartDate'] - health_data['DOB']).dt.days / 365.25\n",
    "\n",
    "# Extrat Year, Month and Day from collumn with dates\n",
    "health_data['ClaimStartDate'] = pd.to_datetime(health_data['ClaimStartDate'], errors='coerce')\n",
    "health_data['ClaimStartYear'] = health_data['ClaimStartDate'].dt.year\n",
    "health_data['ClaimStartMonth'] = health_data['ClaimStartDate'].dt.month\n",
    "health_data['ClaimStartDay'] = health_data['ClaimStartDate'].dt.day\n",
    "\n",
    "health_data['ClaimEndDate'] = pd.to_datetime(health_data['ClaimEndDate'], errors='coerce')\n",
    "health_data['ClaimEndYear'] = health_data['ClaimEndDate'].dt.year\n",
    "health_data['ClaimEndMonth'] = health_data['ClaimEndDate'].dt.month\n",
    "health_data['ClaimEndDay'] = health_data['ClaimEndDate'].dt.day\n",
    "\n",
    "\n",
    "# Drop columns with high number of missing values and unused columns\n",
    "columns_to_drop = ['DOD', 'OperatingID', 'OtherID', 'CodeProcedure', 'EnterDate', 'ExitDate', 'StayDuration', 'ID', 'ClaimStartDate', 'ClaimEndDate', 'DOB']\n",
    "health_data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Encode categorical variables using label encoding\n",
    "categorical_columns = ['ProviderID', 'PolicyholderID', 'Sex', 'AttendingID', 'State', 'CodeFirst', 'CodeSecond', \n",
    "                       'CodeThird', 'CodeHospital']\n",
    "\n",
    "health_data = pd.get_dummies(health_data, columns=categorical_columns)\n",
    "\n",
    "# Checking the dataset after preprocessing\n",
    "health_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X = health_data.drop('ProbableFraud', axis=1)  # Features\n",
    "y = health_data['ProbableFraud']  # Target variable\n",
    "\n",
    "# Performing the split with a 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Training the model on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Model performance metrics\n",
    "accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "243 fits failed out of a total of 486.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "97 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "146 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.42972524 0.42313405 0.42506531\n",
      " 0.42923369 0.42779219 0.42975082 0.42268131 0.42669745 0.42500855\n",
      " 0.42600098 0.42619334 0.4268656  0.43028953 0.42766833 0.43007658\n",
      " 0.42461243 0.42491327 0.42385475 0.42358131 0.42403523 0.4252367\n",
      " 0.42358131 0.42403523 0.4252367  0.42924339 0.42843953 0.43029357\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.56862013 0.57089195 0.56968192\n",
      " 0.5687426  0.56932914 0.57154434 0.57528743 0.57344733 0.57333686\n",
      " 0.56636725 0.57433959 0.57519626 0.56986843 0.56942097 0.57213058\n",
      " 0.5710896  0.56953687 0.56929334 0.5670971  0.56858379 0.56918853\n",
      " 0.5670971  0.56858379 0.56918853 0.56872255 0.56610459 0.56576112\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.60274178 0.61947629 0.62060212\n",
      " 0.61101457 0.61801911 0.62142797 0.60863545 0.61716818 0.6200589\n",
      " 0.60915376 0.61750347 0.6158793  0.60431363 0.60943268 0.61496372\n",
      " 0.61098579 0.61208817 0.61439243 0.60253134 0.60585014 0.60657203\n",
      " 0.60253134 0.60585014 0.60657203 0.60104094 0.60594298 0.60701908]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 150}\n",
      "Best F1 score: 0.6214279748243395\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],     # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],        # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],    # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],      # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt']    # Number of features to consider at every split\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use F1 score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring=f1_scorer)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding F1 score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 score:\", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
